{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb9b52af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Client initialized successfully\n",
      "\n",
      "--- Querying Prediction Guard for available models ---\n",
      "‚úÖ Successfully retrieved 10 available models. Displaying full information...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>object</th>\n",
       "      <th>created</th>\n",
       "      <th>owned_by</th>\n",
       "      <th>description</th>\n",
       "      <th>max_context_length</th>\n",
       "      <th>prompt_format</th>\n",
       "      <th>caps_chat_completion</th>\n",
       "      <th>caps_chat_with_image</th>\n",
       "      <th>caps_completion</th>\n",
       "      <th>caps_embedding</th>\n",
       "      <th>caps_embedding_with_image</th>\n",
       "      <th>caps_tokenize</th>\n",
       "      <th>caps_detokenize</th>\n",
       "      <th>caps_rerank</th>\n",
       "      <th>caps_tool_calling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bge-m3</td>\n",
       "      <td>model</td>\n",
       "      <td>1730332800</td>\n",
       "      <td>Beijing Academy of Artificial Intelligence</td>\n",
       "      <td>BGE M3 is distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Granularity.</td>\n",
       "      <td>8192</td>\n",
       "      <td>none</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bge-reranker-v2-m3</td>\n",
       "      <td>model</td>\n",
       "      <td>1730332800</td>\n",
       "      <td>Beijing Academy of Artificial Intelligence</td>\n",
       "      <td>BGE Reranker v2 M3 is distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Granularity.</td>\n",
       "      <td>8192</td>\n",
       "      <td>none</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bridgetower-large-itm-mlm-itc</td>\n",
       "      <td>model</td>\n",
       "      <td>1730332800</td>\n",
       "      <td>BridgeTower</td>\n",
       "      <td>BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning</td>\n",
       "      <td>8192</td>\n",
       "      <td>none</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DeepSeek-R1-Distill-Qwen-32B</td>\n",
       "      <td>model</td>\n",
       "      <td>1730332800</td>\n",
       "      <td>Deepseek</td>\n",
       "      <td>Deepseek R1 is a family of open-source large language models (LLMs) designed for high-quality code generation and understanding tasks.</td>\n",
       "      <td>20480</td>\n",
       "      <td>none</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hermes-3-Llama-3.1-70B</td>\n",
       "      <td>model</td>\n",
       "      <td>1730332800</td>\n",
       "      <td>NousResearch</td>\n",
       "      <td>Hermes 3 is a generalist language model based on Llama 3.1 70B.</td>\n",
       "      <td>20480</td>\n",
       "      <td>none</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hermes-3-Llama-3.1-8B</td>\n",
       "      <td>model</td>\n",
       "      <td>1730332800</td>\n",
       "      <td>NousResearch</td>\n",
       "      <td>Hermes 3 is a generalist language model based on Llama 3.1 8B.</td>\n",
       "      <td>32768</td>\n",
       "      <td>none</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>multilingual-e5-large-instruct</td>\n",
       "      <td>model</td>\n",
       "      <td>1742828621</td>\n",
       "      <td>intfloat</td>\n",
       "      <td>Open-source multilingual text embeddings model.</td>\n",
       "      <td>512</td>\n",
       "      <td>none</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neural-chat-7b-v3-3</td>\n",
       "      <td>model</td>\n",
       "      <td>1730332800</td>\n",
       "      <td>Intel</td>\n",
       "      <td>Neural Chat is an open-source A fine-tuned model based on Mistral with good coverage of domain and language.</td>\n",
       "      <td>32768</td>\n",
       "      <td>none</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Qwen2.5-Coder-14B-Instruct</td>\n",
       "      <td>model</td>\n",
       "      <td>1730332800</td>\n",
       "      <td>Qwen</td>\n",
       "      <td>Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen).</td>\n",
       "      <td>20480</td>\n",
       "      <td>none</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Qwen2.5-VL-7B-Instruct</td>\n",
       "      <td>model</td>\n",
       "      <td>1730332800</td>\n",
       "      <td>llava hugging face</td>\n",
       "      <td>Open-source multimodal chatbot trained by fine-tuning LLaMa/Vicuna.</td>\n",
       "      <td>16384</td>\n",
       "      <td>none</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id object     created  \\\n",
       "0                          bge-m3  model  1730332800   \n",
       "1              bge-reranker-v2-m3  model  1730332800   \n",
       "2   bridgetower-large-itm-mlm-itc  model  1730332800   \n",
       "3    DeepSeek-R1-Distill-Qwen-32B  model  1730332800   \n",
       "4          Hermes-3-Llama-3.1-70B  model  1730332800   \n",
       "5           Hermes-3-Llama-3.1-8B  model  1730332800   \n",
       "6  multilingual-e5-large-instruct  model  1742828621   \n",
       "7             neural-chat-7b-v3-3  model  1730332800   \n",
       "8      Qwen2.5-Coder-14B-Instruct  model  1730332800   \n",
       "9          Qwen2.5-VL-7B-Instruct  model  1730332800   \n",
       "\n",
       "                                     owned_by  \\\n",
       "0  Beijing Academy of Artificial Intelligence   \n",
       "1  Beijing Academy of Artificial Intelligence   \n",
       "2                                 BridgeTower   \n",
       "3                                    Deepseek   \n",
       "4                                NousResearch   \n",
       "5                                NousResearch   \n",
       "6                                    intfloat   \n",
       "7                                       Intel   \n",
       "8                                        Qwen   \n",
       "9                          llava hugging face   \n",
       "\n",
       "                                                                                                                              description  \\\n",
       "0                            BGE M3 is distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Granularity.   \n",
       "1                BGE Reranker v2 M3 is distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Granularity.   \n",
       "2                                               BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning   \n",
       "3  Deepseek R1 is a family of open-source large language models (LLMs) designed for high-quality code generation and understanding tasks.   \n",
       "4                                                                         Hermes 3 is a generalist language model based on Llama 3.1 70B.   \n",
       "5                                                                          Hermes 3 is a generalist language model based on Llama 3.1 8B.   \n",
       "6                                                                                         Open-source multilingual text embeddings model.   \n",
       "7                            Neural Chat is an open-source A fine-tuned model based on Mistral with good coverage of domain and language.   \n",
       "8                            Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen).   \n",
       "9                                                                     Open-source multimodal chatbot trained by fine-tuning LLaMa/Vicuna.   \n",
       "\n",
       "   max_context_length prompt_format  caps_chat_completion  \\\n",
       "0                8192          none                 False   \n",
       "1                8192          none                 False   \n",
       "2                8192          none                 False   \n",
       "3               20480          none                  True   \n",
       "4               20480          none                  True   \n",
       "5               32768          none                  True   \n",
       "6                 512          none                 False   \n",
       "7               32768          none                  True   \n",
       "8               20480          none                  True   \n",
       "9               16384          none                  True   \n",
       "\n",
       "   caps_chat_with_image  caps_completion  caps_embedding  \\\n",
       "0                 False            False            True   \n",
       "1                 False            False           False   \n",
       "2                 False            False            True   \n",
       "3                 False             True           False   \n",
       "4                 False             True           False   \n",
       "5                 False             True           False   \n",
       "6                 False            False            True   \n",
       "7                 False             True           False   \n",
       "8                 False             True           False   \n",
       "9                  True             True           False   \n",
       "\n",
       "   caps_embedding_with_image  caps_tokenize  caps_detokenize  caps_rerank  \\\n",
       "0                      False           True            False        False   \n",
       "1                      False          False            False         True   \n",
       "2                       True          False            False        False   \n",
       "3                      False           True            False        False   \n",
       "4                      False           True            False        False   \n",
       "5                      False           True            False        False   \n",
       "6                      False           True            False        False   \n",
       "7                      False           True            False        False   \n",
       "8                      False           True            False        False   \n",
       "9                      False           True            False        False   \n",
       "\n",
       "   caps_tool_calling  \n",
       "0              False  \n",
       "1              False  \n",
       "2              False  \n",
       "3              False  \n",
       "4               True  \n",
       "5               True  \n",
       "6              False  \n",
       "7              False  \n",
       "8              False  \n",
       "9              False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from predictionguard import PredictionGuard\n",
    "from IPython.display import display\n",
    "\n",
    "# Initialize and perform deep model exploration \n",
    "    \n",
    "# --- 1. Initialize Prediction Guard client ---\n",
    "try:\n",
    "    # Ensure config.json is in the same directory as this new .ipynb file\n",
    "    with open(\"config.json\", \"r\") as f:\n",
    "        api_config = json.load(f)\n",
    "    os.environ[\"PREDICTIONGUARD_API_KEY\"] = api_config[\"PREDICTIONGUARD_API_KEY\"]\n",
    "    client = PredictionGuard()\n",
    "    print(\"‚úÖ Client initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Initialization failed: {e}\")\n",
    "    client = None\n",
    "\n",
    "# --- 2. Deep exploration: Get and display full information for all models ---\n",
    "if client:\n",
    "    print(\"\\n--- Querying Prediction Guard for available models ---\")\n",
    "    try:\n",
    "        # Get raw data of the model list\n",
    "        models_data = client.models.list().get('data', [])\n",
    "        if models_data:\n",
    "            models_df = pd.DataFrame(models_data)\n",
    "            \n",
    "            # Expand 'capabilities' field for easier viewing\n",
    "            if 'capabilities' in models_df.columns:\n",
    "                caps_df = models_df['capabilities'].apply(pd.Series).add_prefix('caps_')\n",
    "                models_df = pd.concat([models_df.drop('capabilities', axis=1), caps_df], axis=1)\n",
    "\n",
    "            print(f\"‚úÖ Successfully retrieved {len(models_df)} available models. Displaying full information...\")\n",
    "\n",
    "            # Set pandas to display all columns and content for easy viewing\n",
    "            pd.set_option('display.max_columns', None)\n",
    "            pd.set_option('display.max_rows', None)\n",
    "            pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "            # Display the complete DataFrame\n",
    "            display(models_df)\n",
    "        else:\n",
    "            print(\"--- ‚ö†Ô∏è Failed to retrieve any model data from API ---\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error querying model list: {e}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è 'client' object not initialized, unable to perform model exploration. Please check the API key and configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6670b63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 24 examples for evaluation\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Hermes-3-Llama-3.1-8B: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24/24 [07:16<00:00, 18.17s/it]\n",
      "Evaluating Hermes-3-Llama-3.1-70B: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24/24 [05:26<00:00, 13.62s/it]\n",
      "Evaluating DeepSeek-R1-Distill-Qwen-32B: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24/24 [11:17<00:00, 28.22s/it]\n",
      "Evaluating neural-chat-7b-v3-3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24/24 [04:38<00:00, 11.60s/it]\n",
      "Evaluating Qwen2.5-Coder-14B-Instruct: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24/24 [04:09<00:00, 10.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Per-example results saved to facts_grounding_results-2.csv\n",
      "\n",
      "=== Leaderboard ===\n",
      "                       model avg_score pass_rate  n\n",
      "DeepSeek-R1-Distill-Qwen-32B     0.855     83.3% 24\n",
      "  Qwen2.5-Coder-14B-Instruct     0.855     87.5% 24\n",
      "         neural-chat-7b-v3-3     0.854     83.3% 24\n",
      "       Hermes-3-Llama-3.1-8B     0.854     87.5% 24\n",
      "      Hermes-3-Llama-3.1-70B     0.844     83.3% 24\n",
      "\n",
      "üèÅ Leaderboard saved to leaderboard.csv\n",
      "\n",
      "‚è±Ô∏è  Done in 32.8 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Evaluate factuality of model generations on FACTS Grounding 1.0 Public (860 examples).\n",
    "--------------------------------------------------------------------    \n",
    "1) Load dataset (examples.csv)\n",
    "2) For each sample:\n",
    "      ‚Ä¢ Build messages = [system_instruction, user_request]\n",
    "      ‚Ä¢ Call PG chat API ‚Üí get answer\n",
    "      ‚Ä¢ Call PG factuality API (or optional Judge LLM) ‚Üí get score\n",
    "3) Aggregate metrics & write results to CSV\n",
    "--------------------------------------------------------------------\n",
    "All configurable items are grouped in CONFIG dict at top.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from predictionguard import PredictionGuard\n",
    "\n",
    "# --------------------------- CONFIGURATION --------------------------- #\n",
    "CONFIG = {\n",
    "    # --- paths ------------------------------------------------------- #\n",
    "    \"DATA_PATH\":      \"examples.csv\",\n",
    "    \"OUTPUT_CSV\":     \"facts_grounding_results-2.csv\",\n",
    "\n",
    "    # --- Prediction Guard ------------------------------------------- #\n",
    "    \"CONFIG_FILE\":    \"config.json\",\n",
    "    \"MODEL_NAMES\": [\n",
    "    \"Hermes-3-Llama-3.1-8B\",\n",
    "    \"Hermes-3-Llama-3.1-70B\",\n",
    "    \"DeepSeek-R1-Distill-Qwen-32B\",\n",
    "    \"neural-chat-7b-v3-3\"\n",
    "    ],      # generation model\n",
    "    \"TEMPERATURE\":    0.2,\n",
    "    \"MAX_TOKENS\":     512,\n",
    "\n",
    "    # --- factuality -------------------------------------------------- #\n",
    "    \"FACT_THRESHOLD\": 0.80,        # score ‚â• threshold is treated as grounded\n",
    "    \"USE_SECOND_JUDGE\": False,     # set True to enable LLM-as-Judge\n",
    "    \"JUDGE_MODEL_NAME\": \"Qwen2.5-Coder-14B-Instruct\",\n",
    "\n",
    "     # NEW ‚ûú how many examples to evaluate (0 = all)\n",
    "    \"N_SAMPLES\": 24,          # ‚Üê Change to 0 or None to run all\n",
    "}\n",
    "\n",
    "# -------------------------------------------------------------------- #\n",
    "\n",
    "# 1) Initialize and load API KEY (Again!)\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    api_config = json.load(f)\n",
    "os.environ[\"PREDICTIONGUARD_API_KEY\"] = api_config[\"PREDICTIONGUARD_API_KEY\"]\n",
    "client = PredictionGuard()\n",
    "\n",
    "# --------------------------- UTILITIES ------------------------------ #\n",
    "from typing import Any\n",
    "\n",
    "def _extract_score(res: Any) -> float:\n",
    "    \"\"\"\n",
    "    Robustly extract 'score' from Prediction Guard factuality response.\n",
    "\n",
    "    Supports all known schemas:\n",
    "      ‚Ä¢ res.checks[0].score                (Old SDK dataclass)\n",
    "      ‚Ä¢ res['checks'][0]['score']          (dict-like)\n",
    "      ‚Ä¢ res['data'][0]['score']            (REST JSON)\n",
    "      ‚Ä¢ res['score']                       (edge case)\n",
    "    \"\"\"\n",
    "    # dataclass / namedtuple style\n",
    "    if hasattr(res, \"checks\"):\n",
    "        check0 = res.checks[0]\n",
    "        return getattr(check0, \"score\", 0.0)\n",
    "\n",
    "    # dict-like schemas\n",
    "    if isinstance(res, dict):\n",
    "        if \"score\" in res:\n",
    "            return res[\"score\"]\n",
    "        if \"checks\" in res and res[\"checks\"]:\n",
    "            return res[\"checks\"][0].get(\"score\", 0.0)\n",
    "        if \"data\" in res and res[\"data\"]:\n",
    "            return res[\"data\"][0].get(\"score\", 0.0)\n",
    "\n",
    "    raise ValueError(f\"Unexpected factuality response schema: {res}\")\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=20),\n",
    "       stop=stop_after_attempt(6))\n",
    "def pg_factuality(client: PredictionGuard, reference: str, text: str) -> float:\n",
    "    \"\"\"\n",
    "    Call Prediction Guard factuality endpoint (create or check) and return score ‚àà [0,1].\n",
    "    The SDK versions differ: some expose .create(), others .check().\n",
    "    \"\"\"\n",
    "    if hasattr(client.factuality, \"create\"):\n",
    "        res = client.factuality.create(reference=reference, text=text)\n",
    "    elif hasattr(client.factuality, \"check\"):\n",
    "        res = client.factuality.check(reference=reference, text=text)\n",
    "    else:\n",
    "        raise AttributeError(\"PredictionGuard.factuality has neither create() nor check()\")\n",
    "    return _extract_score(res)\n",
    "# -------------------------------------------------------------------- #\n",
    "\n",
    "def chunk_text(text: str, max_tokens: int = 14000) -> str:\n",
    "    \"\"\"\n",
    "    Rough tokenizer-agnostic clip to avoid exceeding context limits.\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    return \" \".join(tokens[:max_tokens])\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=20),\n",
    "       stop=stop_after_attempt(6))\n",
    "def pg_chat(client: PredictionGuard,\n",
    "            messages: List[Dict[str, str]],\n",
    "            model: str,\n",
    "            max_tokens: int,\n",
    "            temperature: float) -> str:\n",
    "    \"\"\"\n",
    "    Robust chat wrapper:\n",
    "      ‚Ä¢ Prefer client.chat.completions.create()\n",
    "      ‚Ä¢ Fallback to client.completions.create() (Old SDK)\n",
    "      ‚Ä¢ Parse both dataclass & dict schemas\n",
    "    \"\"\"\n",
    "    # -------- call API -------- #\n",
    "    if hasattr(client, \"chat\"):          # New SDK\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            max_completion_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "    else:                                # Old SDK\n",
    "        # Expand chat messages into prompt (simplest fallback approach)\n",
    "        prompt = \"\\n\".join([m[\"content\"] for m in messages])\n",
    "        resp = client.completions.create(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "    # -------- extract answer -------- #\n",
    "    # dataclass\n",
    "    if hasattr(resp, \"choices\"):\n",
    "        choice0 = resp.choices[0]\n",
    "        if hasattr(choice0, \"message\") and choice0.message:\n",
    "            return choice0.message.content\n",
    "        if hasattr(choice0, \"text\"):     # completions endpoint\n",
    "            return choice0.text\n",
    "\n",
    "    # dict-like\n",
    "    if isinstance(resp, dict):\n",
    "        if resp.get(\"choices\"):\n",
    "            first = resp[\"choices\"][0]\n",
    "            if \"message\" in first:\n",
    "                return first[\"message\"][\"content\"]\n",
    "            if \"text\" in first:\n",
    "                return first[\"text\"]\n",
    "\n",
    "    raise ValueError(f\"Unrecognized chat/completion response schema: {resp}\")\n",
    "# -------------------------------------------------------------------- #\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=20),\n",
    "       stop=stop_after_attempt(6))\n",
    "def pg_judge(client: PredictionGuard, judge_model: str,\n",
    "             question: str, reference: str, answer: str) -> float:\n",
    "    \"\"\"\n",
    "    Optional 2nd-stage Judge LLM: returns score in [0,1].\n",
    "    Prompt asks Judge to give numeric score only.\n",
    "    \"\"\"\n",
    "    judge_prompt = (\n",
    "        \"You are a fact evaluator. \"\n",
    "        \"Compare the ANSWER against the REFERENCE DOCUMENT in detail. \"\n",
    "        \"Return ONLY a JSON object like {\\\"score\\\": <float 0-1>}.\"\n",
    "        \"\\n\\n[REFERENCE DOCUMENT]\\n\"\n",
    "        f\"{reference}\\n\\n[ANSWER]\\n{answer}\\n\"\n",
    "    )\n",
    "    resp = client.chat.completions.create(\n",
    "        model=judge_model,\n",
    "        messages=[{\"role\": \"user\", \"content\": judge_prompt}],\n",
    "        max_completion_tokens=50,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    try:\n",
    "        judge_json = json.loads(resp.choices[0].message.content)\n",
    "        return float(judge_json[\"score\"])\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "# -------------------------------------------------------------------- #\n",
    "\n",
    "def evaluate_single_model(model_name: str,\n",
    "                          df_eval: pd.DataFrame,\n",
    "                          client: PredictionGuard) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run factuality evaluation for one model and return a result DataFrame\n",
    "    with an extra 'model' column.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for idx, row in tqdm(df_eval.iterrows(),\n",
    "                         total=len(df_eval),\n",
    "                         desc=f\"Evaluating {model_name}\"):\n",
    "        context = chunk_text(row[\"context_document\"])\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": row[\"system_instruction\"]},\n",
    "            {\"role\": \"user\",   \"content\": row[\"user_request\"]}\n",
    "        ]\n",
    "\n",
    "        # ---- generation ---- #\n",
    "        try:\n",
    "            answer = pg_chat(client, messages,\n",
    "                             model=model_name,\n",
    "                             max_tokens=CONFIG[\"MAX_TOKENS\"],\n",
    "                             temperature=CONFIG[\"TEMPERATURE\"])\n",
    "        except Exception as e:\n",
    "            answer = f\"[ERROR] {e}\"\n",
    "\n",
    "        # ---- factuality ---- #\n",
    "        score_pg = None\n",
    "        if not answer.startswith(\"[ERROR]\"):\n",
    "            try:\n",
    "                score_pg = pg_factuality(client,\n",
    "                                         reference=context,\n",
    "                                         text=answer)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå factuality API failed: {e}\")\n",
    "\n",
    "        rows.append({\n",
    "            \"id\": idx,\n",
    "            \"model\": model_name,\n",
    "            \"score_pg\": score_pg,\n",
    "            \"grounded_pg\": (score_pg is not None and\n",
    "                            score_pg >= CONFIG[\"FACT_THRESHOLD\"]),\n",
    "            \"answer\": answer\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def main():\n",
    "    # === init PG ===\n",
    "    with open(CONFIG[\"CONFIG_FILE\"], \"r\") as f:\n",
    "        api_cfg = json.load(f)\n",
    "    os.environ[\"PREDICTIONGUARD_API_KEY\"] = api_cfg[\"PREDICTIONGUARD_API_KEY\"]\n",
    "    client = PredictionGuard()\n",
    "\n",
    "    # === load data ===\n",
    "    df_all = pd.read_csv(CONFIG[\"DATA_PATH\"])\n",
    "\n",
    "    if CONFIG[\"N_SAMPLES\"]:\n",
    "        df_all = df_all.sample(CONFIG[\"N_SAMPLES\"], random_state=42)\n",
    "\n",
    "    print(f\"‚úÖ Loaded {len(df_all)} examples for evaluation\\n\")\n",
    "\n",
    "    # === evaluate each model ===\n",
    "    all_results = []\n",
    "    for m in CONFIG[\"MODEL_NAMES\"]:\n",
    "        res_df = evaluate_single_model(m, df_all, client)\n",
    "        all_results.append(res_df)\n",
    "\n",
    "    final_df = pd.concat(all_results, ignore_index=True)\n",
    "    final_df.to_csv(CONFIG[\"OUTPUT_CSV\"], index=False)\n",
    "    print(f\"\\nüìÑ Per-example results saved to {CONFIG['OUTPUT_CSV']}\")\n",
    "\n",
    "    # === build leaderboard ===\n",
    "    leaderboard = (final_df\n",
    "                   .dropna(subset=[\"score_pg\"])        # exclude failed samples\n",
    "                   .groupby(\"model\")\n",
    "                   .agg(avg_score=(\"score_pg\", \"mean\"),\n",
    "                        pass_rate=(\"grounded_pg\", \"mean\"),\n",
    "                        n=(\"score_pg\", \"count\"))\n",
    "                   .assign(pass_rate=lambda d: d[\"pass_rate\"] * 100)\n",
    "                   .sort_values(\"avg_score\", ascending=False)\n",
    "                   .reset_index())\n",
    "\n",
    "    lb_file = \"leaderboard.csv\"\n",
    "    leaderboard.to_csv(lb_file, index=False)\n",
    "\n",
    "    print(\"\\n=== Leaderboard ===\")\n",
    "    print(leaderboard.to_string(index=False, \n",
    "                                formatters={\n",
    "                                    \"avg_score\": \"{:.3f}\".format,\n",
    "                                    \"pass_rate\": \"{:.1f}%\".format\n",
    "                                }))\n",
    "    print(f\"\\nüèÅ Leaderboard saved to {lb_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tic = time.time()\n",
    "    main()\n",
    "    print(f\"\\n‚è±Ô∏è  Done in {(time.time() - tic)/60:.1f} min\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myPython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
