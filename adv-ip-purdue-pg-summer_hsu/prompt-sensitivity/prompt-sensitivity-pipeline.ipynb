{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6088e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Folder 'prompt-sensitivity' already exists. No need to download.\n",
      "\n",
      "Reading and grouping data from 'prompt-sensitivity/prompt_set/hotpotqa/hotpotqa_mistral_dataset_test.jsonl'...\n",
      "Found 3441 unique prompt groups in the file.\n",
      "Randomly sampling 50 groups.\n",
      "✅ Successfully loaded and sampled 50 prompt groups.\n",
      "\n",
      "--- Sample Preview (All Variants of the First Prompt Group) ---\n",
      "  Variant 1: Can you identify a film released in 2017 that features a character with the first name of the sea and a surname that matches the body part being referred to\n",
      "  Variant 2: what was this actor's final appearance in a cinematic work, which occurred during that same year?\n",
      "What is the title of a 2017 movie that shares its name with a specific body part\n",
      "  Variant 3: what role was last performed by Bill Paxton before his passing?\n",
      "Can you find a film from 2017 whose title matches the word for an indentation below one's belly button\n",
      "  Variant 4: in this film, which actor gave their final screen appearance?\n",
      "What is the name of a movie released in 2017 that shares its name with a navel, as per Dave Evers\n",
      "  Variant 5: what was Bill Paxton's last on-screen role?\n",
      "Can you identify a 2017 cinematic work whose title matches a body part and also corresponds to the name given by Dave Evers for this particular feature\n",
      "  Variant 6: which actor's final performance occurred within this same release year?\n",
      "What film from 2017 shares its title with the sea and also corresponds to the navel, according to Dave Evers\n",
      "  Variant 7: what was Bill Paxton's last acting credit before his death?\n",
      "In a 2017 film, the character has a name that is also the sea, and also refers to the belly button\n",
      "  Variant 8: which actor performed their final role in this work before passing away?\n",
      "What cinematic work released in 2017 has its title matching both the first name of the sea and the word for a body part\n",
      "  Variant 9: what was Bill Paxton's last on-screen performance during that same year?\n",
      "Can you identify a film from 2017 whose title is a specific body part and also the name given by Dave Evers to this particular feature\n",
      "  Variant 10: What is the name of the 2017 movie, which is also the name of the navel by Dave Evers was the last performance by Bill Paxton?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from typing import List, Dict\n",
    "\n",
    "# ===================================================================\n",
    "# Phase 2, Step 1: Understanding and Preparing the Data    \n",
    "# ===================================================================\n",
    "\n",
    "# --- 1. Download the dataset from GitHub ---\n",
    "# Use '!' to execute shell commands in Jupyter Notebook\n",
    "# Download only if the folder does not exist to avoid duplication\n",
    "repo_url = \"https://github.com/Narabzad/prompt-sensitivity.git\"\n",
    "repo_dir = \"prompt-sensitivity\"\n",
    "\n",
    "if not os.path.exists(repo_dir):\n",
    "    print(f\"Folder '{repo_dir}' does not exist. Downloading from GitHub...\")\n",
    "    !git clone {repo_url}\n",
    "    print(\"✅ Dataset downloaded.\")\n",
    "else:\n",
    "    print(f\"✅ Folder '{repo_dir}' already exists. No need to download.\")\n",
    "\n",
    "\n",
    "# --- 2. Write data loading and sampling function (v2: correct grouping version) ---\n",
    "def load_sensitivity_dataset(file_path: str, sample_size: int) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Read data from a .jsonl file, group prompts by 'reference_prompt_id',\n",
    "    and then randomly sample from the groups.\n",
    "\n",
    "    :param file_path: Path to the .jsonl file.\n",
    "    :param sample_size: Number of prompt groups to sample.\n",
    "    :return: A list where each element is a list of prompt variants.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"❌ Error: File not found {file_path}\")\n",
    "        return []\n",
    "\n",
    "    print(f\"\\nReading and grouping data from '{file_path}'...\")\n",
    "    \n",
    "    # Use a dictionary to group prompts; key is reference_prompt_id, value is a list of prompts\n",
    "    grouped_prompts = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                ref_id = data.get('reference_prompt_id')\n",
    "                prompt_text = data.get('prompt')\n",
    "\n",
    "                if ref_id and prompt_text:\n",
    "                    if ref_id not in grouped_prompts:\n",
    "                        grouped_prompts[ref_id] = []\n",
    "                    grouped_prompts[ref_id].append(prompt_text)\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                # Skip lines with format errors\n",
    "                continue\n",
    "    \n",
    "    if not grouped_prompts:\n",
    "        print(\"⚠️ Warning: Failed to build any prompt groups from the file.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"Found {len(grouped_prompts)} unique prompt groups in the file.\")\n",
    "    \n",
    "    # Sample from the grouped prompts\n",
    "    all_group_ids = list(grouped_prompts.keys())\n",
    "    sample_size = min(sample_size, len(all_group_ids))\n",
    "    print(f\"Randomly sampling {sample_size} groups.\")\n",
    "    \n",
    "    random.seed(42) # For reproducibility\n",
    "    sampled_group_ids = random.sample(all_group_ids, sample_size)\n",
    "    \n",
    "    # Build the final prompt list from the sampled group IDs\n",
    "    final_prompt_groups = [grouped_prompts[gid] for gid in sampled_group_ids]\n",
    "    \n",
    "    print(f\"✅ Successfully loaded and sampled {len(final_prompt_groups)} prompt groups.\")\n",
    "    return final_prompt_groups\n",
    "\n",
    "\n",
    "# --- 3. Execute and inspect the data ---\n",
    "# This time we increase the sample size to 50 groups\n",
    "SAMPLE_SIZE = 50\n",
    "hotpotqa_test_path = os.path.join(repo_dir, \"prompt_set\", \"hotpotqa\", \"hotpotqa_mistral_dataset_test.jsonl\")\n",
    "hotpotqa_prompt_groups = load_sensitivity_dataset(hotpotqa_test_path, SAMPLE_SIZE)\n",
    "\n",
    "# Print all variants of the first prompt group\n",
    "if hotpotqa_prompt_groups:\n",
    "    print(\"\\n--- Sample Preview (All Variants of the First Prompt Group) ---\")\n",
    "    for i, prompt in enumerate(hotpotqa_prompt_groups[0]):\n",
    "        print(f\"  Variant {i+1}: {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f48fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking file: prompt-sensitivity/prompt_set/hotpotqa/hotpotqa_mistral_dataset_test.jsonl\n",
      "\n",
      "--- Raw text of the first line in the file ---\n",
      "{\"prompt_id\": \"alt_hpqa_f6a03027_1\", \"reference_prompt_id\": \"hpqa_f6a03027\", \"prompt\": \"In the same metropolitan area as the Taj Mahal\", \"expected_answer\": [\"Delhi\"], \"model_answers\": {\"llama3.1:8b\": \"Agra India.\", \"mistral-nemo:latest\": \"Agra\"}, \"llm_response\": \"Agra\", \"exact_match\": 0, \"model\": \"mistral-nemo\"}\n",
      "\n",
      "\n",
      "--- Keys after parsing the line as JSON ---\n",
      "dict_keys(['prompt_id', 'reference_prompt_id', 'prompt', 'expected_answer', 'model_answers', 'llm_response', 'exact_match', 'model'])\n"
     ]
    }
   ],
   "source": [
    "# Dataset JSON debug block: check the actual structure of a single line\n",
    "import json\n",
    "\n",
    "# ===================================================================\n",
    "# Debug block: check the actual structure of a single line\n",
    "# ===================================================================\n",
    "file_path = \"prompt-sensitivity/prompt_set/hotpotqa/hotpotqa_mistral_dataset_test.jsonl\"\n",
    "print(f\"Checking file: {file_path}\\n\")\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r') as f:\n",
    "        # Read only the first line of the file for analysis\n",
    "        first_line = f.readline()\n",
    "\n",
    "        print(\"--- Raw text of the first line in the file ---\")\n",
    "        print(first_line)\n",
    "\n",
    "        print(\"\\n--- Keys after parsing the line as JSON ---\")\n",
    "        # Parse the single line into a Python dictionary\n",
    "        data = json.loads(first_line)\n",
    "        # Print all keys in this dictionary\n",
    "        print(data.keys())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cb13ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myPython/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Prediction Guard client initialized successfully.\n",
      "\n",
      "Reading and grouping data from 'prompt-sensitivity/prompt_set/hotpotqa/hotpotqa_mistral_dataset_test.jsonl'...\n",
      "Found 3441 distinct prompt groups in the file.\n",
      "Randomly sampling 50 groups.\n",
      "✅ Successfully loaded and sampled 50 prompt groups.\n",
      "\n",
      "▶️ Starting evaluation for model: DeepSeek-R1-Distill-Qwen-32B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Evaluation Progress (DeepSeek-R1-Distill-Qwen-32B): 100%|██████████| 50/50 [53:32<00:00, 64.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📊 Average robustness score for DeepSeek-R1-Distill-Qwen-32B = 0.7039\n",
      "\n",
      "▶️ Starting evaluation for model: Hermes-3-Llama-3.1-70B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Evaluation Progress (Hermes-3-Llama-3.1-70B): 100%|██████████| 50/50 [33:03<00:00, 39.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📊 Average robustness score for Hermes-3-Llama-3.1-70B = 0.6974\n",
      "\n",
      "▶️ Starting evaluation for model: Hermes-3-Llama-3.1-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Evaluation Progress (Hermes-3-Llama-3.1-8B): 100%|██████████| 50/50 [12:43<00:00, 15.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📊 Average robustness score for Hermes-3-Llama-3.1-8B = 0.6323\n",
      "\n",
      "▶️ Starting evaluation for model: neural-chat-7b-v3-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Evaluation Progress (neural-chat-7b-v3-3): 100%|██████████| 50/50 [16:27<00:00, 19.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📊 Average robustness score for neural-chat-7b-v3-3 = 0.6520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "--- \n",
       " ## 📊 Prompt Sensitivity Robustness Leaderboard"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "`robustness_score` (robustness) **higher is better**, meaning the model’s responses are less affected by minor prompt variations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>robustness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DeepSeek-R1-Distill-Qwen-32B</td>\n",
       "      <td>0.703900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hermes-3-Llama-3.1-70B</td>\n",
       "      <td>0.697386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neural-chat-7b-v3-3</td>\n",
       "      <td>0.651963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hermes-3-Llama-3.1-8B</td>\n",
       "      <td>0.632337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          model  robustness_score\n",
       "0  DeepSeek-R1-Distill-Qwen-32B          0.703900\n",
       "1        Hermes-3-Llama-3.1-70B          0.697386\n",
       "2           neural-chat-7b-v3-3          0.651963\n",
       "3         Hermes-3-Llama-3.1-8B          0.632337"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import time  # <--- 【Fix 1】: Added import time\n",
    "from typing import List, Dict\n",
    "from itertools import combinations\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from predictionguard import PredictionGuard\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "\n",
    "# ===================================================================\n",
    "# Phase 2, Full Pipeline: Setup, Testing, and Scoring (Revised)\n",
    "# ===================================================================\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "SENSITIVITY_CONFIG = {\n",
    "    \"DATA_FILE_PATH\": \"prompt-sensitivity/prompt_set/hotpotqa/hotpotqa_mistral_dataset_test.jsonl\",\n",
    "    \"SAMPLE_SIZE\": 50,\n",
    "    \"CHAT_MODELS_TO_TEST\": [\n",
    "        \"DeepSeek-R1-Distill-Qwen-32B\",\n",
    "        \"Hermes-3-Llama-3.1-70B\",\n",
    "        \"Hermes-3-Llama-3.1-8B\",\n",
    "        \"neural-chat-7b-v3-3\"\n",
    "    ],\n",
    "    \"EMBEDDING_MODEL_ID\": \"bge-m3\",\n",
    "    \"API_MAX_TOKENS\": 128,\n",
    "    \"API_SLEEP_INTERVAL\": 0.6,\n",
    "}\n",
    "\n",
    "# --- 2. Data-loading function (unchanged) ---\n",
    "def load_sensitivity_dataset(file_path: str, sample_size: int) -> List[List[str]]:\n",
    "    # ... (function body unchanged)\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"❌ Error: File not found {file_path}\"); return []\n",
    "    \n",
    "    print(f\"\\nReading and grouping data from '{file_path}'...\")\n",
    "    grouped_prompts = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                ref_id, prompt_text = data.get('reference_prompt_id'), data.get('prompt')\n",
    "                if ref_id and prompt_text:\n",
    "                    if ref_id not in grouped_prompts: grouped_prompts[ref_id] = []\n",
    "                    grouped_prompts[ref_id].append(prompt_text)\n",
    "            except json.JSONDecodeError: continue\n",
    "            \n",
    "    print(f\"Found {len(grouped_prompts)} distinct prompt groups in the file.\")\n",
    "    all_group_ids = list(grouped_prompts.keys())\n",
    "    sample_size = min(sample_size, len(all_group_ids))\n",
    "    print(f\"Randomly sampling {sample_size} groups.\")\n",
    "    \n",
    "    random.seed(42)\n",
    "    sampled_group_ids = random.sample(all_group_ids, sample_size)\n",
    "    final_prompt_groups = [grouped_prompts[gid] for gid in sampled_group_ids]\n",
    "    \n",
    "    print(f\"✅ Successfully loaded and sampled {len(final_prompt_groups)} prompt groups.\")\n",
    "    return final_prompt_groups\n",
    "\n",
    "# --- 3. Prompt Sensitivity Tester Class (v2 Revised) ---\n",
    "class PromptSensitivityTester:\n",
    "    def __init__(self, client: PredictionGuard, config: Dict):\n",
    "        self.client = client\n",
    "        self.config = config\n",
    "        self.embedding_model = config[\"EMBEDDING_MODEL_ID\"]\n",
    "\n",
    "    def _extract_content(self, resp: any) -> str:\n",
    "        # (function remains unchanged)\n",
    "        try:\n",
    "            if isinstance(resp, dict):\n",
    "                return resp.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "            elif hasattr(resp, \"choices\") and len(resp.choices) > 0:\n",
    "                return resp.choices[0].message.content\n",
    "            else:\n",
    "                return str(resp)\n",
    "        except (AttributeError, IndexError, KeyError):\n",
    "            return f\"ERROR: Unrecognized response format {str(resp)}\"\n",
    "\n",
    "    def get_responses(self, model_id: str, prompt_group: List[str]) -> List[str]:\n",
    "        # (function remains unchanged)\n",
    "        responses = []\n",
    "        for prompt in prompt_group:\n",
    "            try:\n",
    "                resp = self.client.chat.completions.create(\n",
    "                    model=model_id,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_completion_tokens=self.config[\"API_MAX_TOKENS\"],\n",
    "                )\n",
    "                content = self._extract_content(resp)\n",
    "                responses.append(content)\n",
    "            except Exception as e:\n",
    "                error_msg = f\"ERROR: {e}\"\n",
    "                print(f\"  ⚠️ API request failed: {error_msg}\")\n",
    "                responses.append(error_msg)\n",
    "            time.sleep(self.config[\"API_SLEEP_INTERVAL\"])\n",
    "        return responses\n",
    "\n",
    "    def get_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"【【【 Fixed function 】】】Convert a list of texts into embedding vectors.\"\"\"\n",
    "        try:\n",
    "            valid_texts = [t for t in texts if t and not t.startswith(\"ERROR:\")]\n",
    "            if not valid_texts: return np.array([])\n",
    "\n",
    "            # Fix: use client.embeddings (plural) instead of client.embedding\n",
    "            resp = self.client.embeddings.create(\n",
    "                model=self.embedding_model,\n",
    "                input=valid_texts\n",
    "            )\n",
    "            # Ensure the returned 'data' is a list\n",
    "            if 'data' in resp and isinstance(resp['data'], list):\n",
    "                 return np.array([item['embedding'] for item in resp['data']])\n",
    "            else:\n",
    "                print(f\"  ⚠️ Embedding API response format unexpected: {resp}\")\n",
    "                return np.array([])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️ Embedding API request failed: {e}\")\n",
    "            return np.array([])\n",
    "\n",
    "    def calculate_avg_similarity(self, embeddings: np.ndarray) -> float:\n",
    "        # (function remains unchanged)\n",
    "        if embeddings.shape[0] < 2: return 0.0\n",
    "        sim_matrix = cosine_similarity(embeddings)\n",
    "        indices = np.triu_indices_from(sim_matrix, k=1)\n",
    "        pairwise_scores = sim_matrix[indices]\n",
    "        return np.mean(pairwise_scores) if pairwise_scores.size > 0 else 1.0\n",
    "\n",
    "    def run_evaluation_for_model(self, model_id: str, all_prompt_groups: List[List[str]]) -> float:\n",
    "        # (function remains unchanged)\n",
    "        model_scores = []\n",
    "        print(f\"\\n▶️ Starting evaluation for model: {model_id}\")\n",
    "        progress_bar = tqdm(all_prompt_groups, desc=f\"  Evaluation Progress ({model_id})\")\n",
    "        for group in progress_bar:\n",
    "            responses = self.get_responses(model_id, group)\n",
    "            embeddings = self.get_embeddings(responses)\n",
    "            if embeddings.size == 0: continue\n",
    "            avg_sim = self.calculate_avg_similarity(embeddings)\n",
    "            model_scores.append(avg_sim)\n",
    "        overall_avg_score = np.mean(model_scores) if model_scores else 0.0\n",
    "        print(f\"  📊 Average robustness score for {model_id} = {overall_avg_score:.4f}\")\n",
    "        return overall_avg_score\n",
    "\n",
    "# --- 4. Main execution flow (unchanged) ---\n",
    "if 'client' not in locals():\n",
    "    try:\n",
    "        with open(\"config.json\", \"r\") as f: config = json.load(f)\n",
    "        os.environ[\"PREDICTIONGUARD_API_KEY\"] = config[\"PREDICTIONGUARD_API_KEY\"]\n",
    "        client = PredictionGuard()\n",
    "        print(\"✅ Prediction Guard client initialized successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Initialization failed: {e}\"); client = None\n",
    "\n",
    "if client:\n",
    "    prompt_groups_for_testing = load_sensitivity_dataset(\n",
    "        SENSITIVITY_CONFIG[\"DATA_FILE_PATH\"], SENSITIVITY_CONFIG[\"SAMPLE_SIZE\"]\n",
    "    )\n",
    "    if prompt_groups_for_testing:\n",
    "        tester = PromptSensitivityTester(client, SENSITIVITY_CONFIG)\n",
    "        leaderboard = []\n",
    "        models_to_test = SENSITIVITY_CONFIG[\"CHAT_MODELS_TO_TEST\"]\n",
    "        for model_id in models_to_test:\n",
    "            score = tester.run_evaluation_for_model(model_id, prompt_groups_for_testing)\n",
    "            leaderboard.append({\"model\": model_id, \"robustness_score\": score})\n",
    "        leaderboard_df = pd.DataFrame(leaderboard).sort_values(\n",
    "            by=\"robustness_score\", ascending=False\n",
    "        ).reset_index(drop=True)\n",
    "        display(Markdown(\"\\n--- \\n ## 📊 Prompt Sensitivity Robustness Leaderboard\"))\n",
    "        display(Markdown(\"`robustness_score` (robustness) **higher is better**, meaning the model’s responses are less affected by minor prompt variations.\"))\n",
    "        display(leaderboard_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myPython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
